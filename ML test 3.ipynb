{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqUtils\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('final file hopefully no errors.csv')\n",
    "\n",
    "#for fib2.cvs only\n",
    "\n",
    "dataset.loc[dataset['Gen'] == 'ssRNA(+)', 'Gen'] = 1\n",
    "dataset.loc[dataset['Gen'] == 'ssRNA(-)', 'Gen'] = 0\n",
    "\n",
    "p = dataset.loc[dataset['Gen'] == 1].reset_index(drop = True)\n",
    "n = dataset.loc[dataset['Gen'] == 0].reset_index(drop = True)\n",
    "\n",
    "pn = pd.concat([p,n]).reset_index(drop = True)\n",
    "res = pd.read_csv('res 340.csv')\n",
    "r = res[['ATC', 'ATCA', 'ATCT', 'TGGT']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "pnr = pnr.rename(columns={'ATC' : 'E1', 'ATCA' : 'E2', 'ATCT' : 'E3', 'TGGT' : 'E4'})\n",
    "\n",
    "def orf_translator(record, table, min_pro_len):\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    for strand, nuc in [(+1, record.seq), (-1, record.seq.reverse_complement())]:\n",
    "        for frame in range(3):\n",
    "            length = 3 * ((len(record)-frame) // 3) #Multiple of three\n",
    "            for pro in nuc[frame:frame+length].translate(table).split(\"*\"):\n",
    "                if len(pro) >= min_pro_len:\n",
    "                    if strand == 1:\n",
    "                        s1.append(pro)\n",
    "                    else:\n",
    "                        s2.append(pro)\n",
    "    return s1,s2\n",
    "\n",
    "\n",
    "def whole(pnr):\n",
    "    \n",
    "    #sampling\n",
    "    ps = pnr.loc[pnr['Gen'] == 1]#.sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0]#.sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9847133757961783\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e6a8b4cd09c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwhole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpnr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0maclist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4a0dd36a8a3e>\u001b[0m in \u001b[0;36mwhole\u001b[1;34m(pnr)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpnrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Seq'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeqRecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morf_translator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4a0dd36a8a3e>\u001b[0m in \u001b[0;36morf_translator\u001b[1;34m(record, table, min_pro_len)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Multiple of three\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mpro\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnuc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpro\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmin_pro_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mstrand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michael\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\Bio\\Seq.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, table, stop_symbol, to_stop, cds, gap)\u001b[0m\n\u001b[0;32m   1275\u001b[0m                 )\n\u001b[0;32m   1276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1277\u001b[1;33m         protein = _translate_str(\n\u001b[0m\u001b[0;32m   1278\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcodon_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_symbol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m         )\n",
      "\u001b[1;32mc:\\users\\michael\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\Bio\\Seq.py\u001b[0m in \u001b[0;36m_translate_str\u001b[1;34m(sequence, table, stop_symbol, to_stop, cds, pos_stop, gap)\u001b[0m\n\u001b[0;32m   2867\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2868\u001b[0m         \u001b[0mcodon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2869\u001b[1;33m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2870\u001b[0m             \u001b[0mamino_acids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforward_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcodon\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2871\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCodonTable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTranslationError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "\n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['GTGG', 'TGGC', 'TGGT', 'CGTG', 'ATC', 'ATCA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['GTGG', 'TGGC', 'TGGT', 'CGTG', 'ATC', 'ATCA']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "pnr = pnr.rename(columns={'GTGG' : 'E1', 'TGGC' : 'E2', 'TGGT' : 'E3', 'CGTG' : 'E4', 'ATC' : 'E5', 'ATCA' : 'E6'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n",
      "0.91\n",
      "0.97\n",
      "0.96\n",
      "0.96\n",
      "0.94\n",
      "0.98\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.93\n",
      "0.96\n",
      "0.97\n",
      "0.96\n",
      "0.95\n",
      "0.92\n",
      "0.94\n",
      "0.96\n",
      "0.94\n",
      "0.9505000000000002\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['ATCT', 'TGGC', 'TGGT', 'CGTG', 'ATC', 'ATCA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['ATCT', 'TGGC', 'TGGT', 'CGTG', 'ATC', 'ATCA']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "pnr = pnr.rename(columns={'ATCT' : 'E1', 'TGGC' : 'E2', 'TGGT' : 'E3', 'CGTG' : 'E4', 'ATC' : 'E5', 'ATCA' : 'E6'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n",
      "0.99\n",
      "0.93\n",
      "0.97\n",
      "0.94\n",
      "0.92\n",
      "0.94\n",
      "0.97\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.97\n",
      "0.95\n",
      "0.93\n",
      "0.95\n",
      "0.96\n",
      "0.95\n",
      "0.97\n",
      "0.94\n",
      "0.9475\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['TGGT','ATC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['TGGT','ATC']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "pnr = pnr.rename(columns={'TGGT' : 'E3', 'ATC' : 'E5'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.91\n",
      "0.97\n",
      "0.94\n",
      "0.98\n",
      "0.93\n",
      "0.94\n",
      "0.92\n",
      "0.94\n",
      "0.92\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.92\n",
      "0.94\n",
      "0.88\n",
      "0.97\n",
      "0.9400000000000001\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['ATC', 'ATCA', 'GATC', 'ATCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['ATC', 'ATCA', 'GATC', 'ATCT']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "#pnr = pnr.rename(columns={'TGGT' : 'E3', 'ATC' : 'E5'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n",
      "0.95\n",
      "0.95\n",
      "0.97\n",
      "0.96\n",
      "0.94\n",
      "0.96\n",
      "0.98\n",
      "0.98\n",
      "0.95\n",
      "0.94\n",
      "0.96\n",
      "0.92\n",
      "0.95\n",
      "0.97\n",
      "0.93\n",
      "0.96\n",
      "0.94\n",
      "0.96\n",
      "0.93\n",
      "0.9515\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['ATC', 'ATCA', 'ATCT', 'TATC', 'TGGT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['ATC', 'ATCA', 'ATCT', 'TATC', 'TGGT']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "#pnr = pnr.rename(columns={'TGGT' : 'E3', 'ATC' : 'E5'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n",
      "0.95\n",
      "0.97\n",
      "0.96\n",
      "0.97\n",
      "0.93\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0.96\n",
      "0.95\n",
      "0.93\n",
      "0.98\n",
      "0.96\n",
      "0.99\n",
      "0.95\n",
      "0.97\n",
      "0.95\n",
      "0.98\n",
      "0.95\n",
      "0.9564999999999999\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['ATC', 'ATCA', 'ATCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['ATC', 'ATCA', 'ATCT']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "#pnr = pnr.rename(columns={'TGGT' : 'E3', 'ATC' : 'E5'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n",
      "0.95\n",
      "0.95\n",
      "0.98\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.97\n",
      "0.94\n",
      "0.93\n",
      "0.97\n",
      "0.93\n",
      "0.99\n",
      "0.9\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.97\n",
      "0.91\n",
      "0.94\n",
      "0.9450000000000001\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['ATC', 'ATCA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['ATC', 'ATCA']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "#pnr = pnr.rename(columns={'TGGT' : 'E3', 'ATC' : 'E5'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n",
      "0.92\n",
      "0.95\n",
      "0.93\n",
      "0.92\n",
      "0.93\n",
      "0.96\n",
      "0.96\n",
      "0.98\n",
      "0.95\n",
      "0.92\n",
      "0.94\n",
      "0.96\n",
      "0.99\n",
      "0.94\n",
      "0.96\n",
      "0.9\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0.9479999999999998\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
