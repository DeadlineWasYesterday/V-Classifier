{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqUtils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('final file hopefully no errors.csv')\n",
    "\n",
    "#for fib2.cvs only\n",
    "\n",
    "dataset.loc[dataset['Gen'] == 'ssRNA(+)', 'Gen'] = 1\n",
    "dataset.loc[dataset['Gen'] == 'ssRNA(-)', 'Gen'] = 0\n",
    "\n",
    "p = dataset.loc[dataset['Gen'] == 1].reset_index(drop = True)\n",
    "n = dataset.loc[dataset['Gen'] == 0].reset_index(drop = True)\n",
    "\n",
    "pn = pd.concat([p,n]).reset_index(drop = True)\n",
    "res = pd.read_csv('res 340.csv')\n",
    "r = res[['ATC', 'ATCA', 'ATCT', 'TGGT']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "#pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "#pnr = pnr.rename(columns={'ATC' : 'E1', 'ATCA' : 'E2', 'ATCT' : 'E3', 'TGGT' : 'E4'})\n",
    "\n",
    "def orf_translator(record, table, min_pro_len):\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    for strand, nuc in [(+1, record.seq), (-1, record.seq.reverse_complement())]:\n",
    "        for frame in range(3):\n",
    "            length = 3 * ((len(record)-frame) // 3) #Multiple of three\n",
    "            for pro in nuc[frame:frame+length].translate(table).split(\"*\"):\n",
    "                if len(pro) >= min_pro_len:\n",
    "                    if strand == 1:\n",
    "                        s1.append(pro)\n",
    "                    else:\n",
    "                        s2.append(pro)\n",
    "    return s1,s2\n",
    "\n",
    "\n",
    "def whole(pnr):\n",
    "    \n",
    "    #sampling\n",
    "    ps = pnr.loc[pnr['Gen'] == 1]\n",
    "    ns = pnr.loc[pnr['Gen'] == 0]\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    scores_mac = cross_val_score(classifier, X, y, cv=5, scoring='f1_macro')\n",
    "    \n",
    "    return scores_mac  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9624808475878766 0.0632894338450859\n",
      "0.9549906220689384 0.04359149716868421\n",
      "0.957402241862148 0.03422942331248438\n",
      "0.9449898392308601 0.046371298778821075\n",
      "0.9699718118550008 0.04363901120221446\n",
      "0.9649859220981686 0.010014143933072362\n",
      "0.9549679285330752 0.04901410130189069\n",
      "0.9574730162579119 0.0300459345683051\n",
      "0.9674384652345042 0.037578507350368974\n",
      "0.9674933568712513 0.012245535152485122\n",
      "0.9524738049668274 0.0186961766053732\n",
      "0.9549577177825013 0.04369296351364044\n",
      "0.9474636045160845 0.01869635785656151\n",
      "0.957483974542223 0.04362020188275059\n",
      "0.9649459663602808 0.06614917334063593\n",
      "0.9399107635376021 0.05346588285945927\n",
      "0.9574839686679452 0.019993768891602367\n",
      "0.952491012754983 0.04848147128319739\n",
      "0.9574964838255978 0.03391418522168333\n",
      "0.9449365957970726 0.034014077987756755\n",
      "0.9565918972175427\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    fmac = whole(pnr)\n",
    "    aclist.append(fmac.mean())\n",
    "    print(fmac.mean(), fmac.std() *2)\n",
    "\n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['GTGG', 'TGGC', 'TGGT', 'CGTG', 'ATC', 'ATCA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['GTGG', 'TGGC', 'TGGT', 'CGTG', 'ATC', 'ATCA']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "pnr = pnr.rename(columns={'GTGG' : 'E1', 'TGGC' : 'E2', 'TGGT' : 'E3', 'CGTG' : 'E4', 'ATC' : 'E5', 'ATCA' : 'E6'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n",
      "0.91\n",
      "0.97\n",
      "0.96\n",
      "0.96\n",
      "0.94\n",
      "0.98\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.93\n",
      "0.96\n",
      "0.97\n",
      "0.96\n",
      "0.95\n",
      "0.92\n",
      "0.94\n",
      "0.96\n",
      "0.94\n",
      "0.9505000000000002\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['ATCT', 'TGGC', 'TGGT', 'CGTG', 'ATC', 'ATCA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['ATCT', 'TGGC', 'TGGT', 'CGTG', 'ATC', 'ATCA']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "pnr = pnr.rename(columns={'ATCT' : 'E1', 'TGGC' : 'E2', 'TGGT' : 'E3', 'CGTG' : 'E4', 'ATC' : 'E5', 'ATCA' : 'E6'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n",
      "0.99\n",
      "0.93\n",
      "0.97\n",
      "0.94\n",
      "0.92\n",
      "0.94\n",
      "0.97\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.97\n",
      "0.95\n",
      "0.93\n",
      "0.95\n",
      "0.96\n",
      "0.95\n",
      "0.97\n",
      "0.94\n",
      "0.9475\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['TGGT','ATC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['TGGT','ATC']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "pnr = pnr.rename(columns={'TGGT' : 'E3', 'ATC' : 'E5'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.91\n",
      "0.97\n",
      "0.94\n",
      "0.98\n",
      "0.93\n",
      "0.94\n",
      "0.92\n",
      "0.94\n",
      "0.92\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.92\n",
      "0.94\n",
      "0.88\n",
      "0.97\n",
      "0.9400000000000001\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['ATC', 'ATCA', 'GATC', 'ATCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['ATC', 'ATCA', 'GATC', 'ATCT']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "#pnr = pnr.rename(columns={'TGGT' : 'E3', 'ATC' : 'E5'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n",
      "0.95\n",
      "0.95\n",
      "0.97\n",
      "0.96\n",
      "0.94\n",
      "0.96\n",
      "0.98\n",
      "0.98\n",
      "0.95\n",
      "0.94\n",
      "0.96\n",
      "0.92\n",
      "0.95\n",
      "0.97\n",
      "0.93\n",
      "0.96\n",
      "0.94\n",
      "0.96\n",
      "0.93\n",
      "0.9515\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['ATC', 'ATCA', 'ATCT', 'TATC', 'TGGT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['ATC', 'ATCA', 'ATCT', 'TATC', 'TGGT']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "#pnr = pnr.rename(columns={'TGGT' : 'E3', 'ATC' : 'E5'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n",
      "0.95\n",
      "0.97\n",
      "0.96\n",
      "0.97\n",
      "0.93\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0.96\n",
      "0.95\n",
      "0.93\n",
      "0.98\n",
      "0.96\n",
      "0.99\n",
      "0.95\n",
      "0.97\n",
      "0.95\n",
      "0.98\n",
      "0.95\n",
      "0.9564999999999999\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['ATC', 'ATCA', 'ATCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['ATC', 'ATCA', 'ATCT']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "#pnr = pnr.rename(columns={'TGGT' : 'E3', 'ATC' : 'E5'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n",
      "0.95\n",
      "0.95\n",
      "0.98\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.97\n",
      "0.94\n",
      "0.93\n",
      "0.97\n",
      "0.93\n",
      "0.99\n",
      "0.9\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.97\n",
      "0.91\n",
      "0.94\n",
      "0.9450000000000001\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['ATC', 'ATCA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['ATC', 'ATCA']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(index = pnr.loc[pnr['MolTyp'] == 'DNA'].index)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "#pnr = pnr.rename(columns={'TGGT' : 'E3', 'ATC' : 'E5'})\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1].sample(200)\n",
    "    ns = pnr.loc[pnr['Gen'] == 0].sample(200)\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n",
      "0.92\n",
      "0.95\n",
      "0.93\n",
      "0.92\n",
      "0.93\n",
      "0.96\n",
      "0.96\n",
      "0.98\n",
      "0.95\n",
      "0.92\n",
      "0.94\n",
      "0.96\n",
      "0.99\n",
      "0.94\n",
      "0.96\n",
      "0.9\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0.9479999999999998\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    cm = whole(pnr)\n",
    "    aclist.append((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    print((cm[0][0] + cm[1][1]) / (cm.sum()))\n",
    "    \n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ['GGCA', 'GCCA', 'ACG', 'TGAT', 'TCTA', 'TTAA', 'ATTC', 'G', 'TAAG', 'TTAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[['GGCA', 'GCCA', 'ACG', 'TGAT', 'TCTA', 'TTAA', 'ATTC', 'G', 'TAAG', 'TTAT']].copy()\n",
    "pnr = pd.concat([pn, r], axis = 1)\n",
    "pnr = pnr.drop(columns=['Id', 'MolTyp', 'Topol', 'Des'])\n",
    "\n",
    "def whole(pnr):\n",
    "\n",
    "    ps = pnr.loc[pnr['Gen'] == 1]\n",
    "    ns = pnr.loc[pnr['Gen'] == 0]\n",
    "\n",
    "    pnrs = pd.concat([ps,ns])\n",
    "    \n",
    "    \n",
    "    #feature 1: length\n",
    "    pnrs['F1'] = [len(i) for i in pnrs['Seq'].to_list()]\n",
    "    \n",
    "    f2, f3, f4, f5, f6, f7 = [],[],[],[],[],[]\n",
    "\n",
    "    for i in pnrs['Seq'].to_list():\n",
    "        a = SeqRecord(Seq(i))\n",
    "        (s1, s2) = orf_translator(a, 11, 15)\n",
    "        f2.append(len(s1))\n",
    "        f3.append(len(s2))\n",
    "\n",
    "    pnrs['F2'] = f2\n",
    "    pnrs['F3'] = f3\n",
    "    \n",
    "    #Splitting features and labels\n",
    "    X = pnrs.iloc[:, 3:].values.astype(float)\n",
    "    y = pnrs.iloc[:, 1].values.astype(float)\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    scores_mac = cross_val_score(classifier, X, y, cv=5, scoring='f1_macro')\n",
    "    \n",
    "    return scores_mac  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653658 0.07213781802967897\n",
      "0.9201008909653657\n"
     ]
    }
   ],
   "source": [
    "aclist = []\n",
    "\n",
    "for i in range(20):\n",
    "    fmac = whole(pnr)\n",
    "    aclist.append(fmac.mean())\n",
    "    print(fmac.mean(), fmac.std() *2)\n",
    "\n",
    "print(np.mean(aclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
